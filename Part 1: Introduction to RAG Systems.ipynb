{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Introduction to RAG Systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demonstrating a simple RAG pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the libraries\n",
    "- We need torch because transformers, the main library used in this notebook needs it\n",
    "- the transformers library, has a pipeline wrapper which takes care of the task of building the embedding generation pipeline, which is a BERT like encoder model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the knowledge corpus\n",
    "- In the current simple example is merely a collection of sentences, in actual cases, it might be documents, which will then have to split using various chunking strategies, which will be discuss in later notebooks\n",
    "- The sentences are chosen at random, but a few sentences might have some sematic similarity to each other, which is relevant for later use cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 1: Define documents\n",
    "knowledge_base = [\n",
    "    \"Metformin is a medication used to treat type 2 diabetes.\",\n",
    "    \"It works by lowering glucose production in the liver.\",\n",
    "    \"Common side effects include nausea, upset stomach, and diarrhea.\",\n",
    "    \"Octopuses have three hearts and blue blood.\",\n",
    "    \"Honey never spoils and has been found edible in ancient Egyptian tombs.\",\n",
    "    \"Sharks have been around longer than trees, existing for over 400 million years.\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Retriever engine\n",
    "- Keeping this simple for now, we have created an example where we just trying to determine the common words between the query and the documents.\n",
    "- If any of our document corpus contains any of words in the query, the document will be included as part of the context for generation.\n",
    "- This approach is very naive, doesn't include pre-processing steps such as stopwords removal, tokenization etc., but that's just to keep things simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Step 2: Define a simple retriever (keyword matching)\n",
    "def retrieve_docs(query, documents):\n",
    "    \n",
    "    matching_docs = []\n",
    "\n",
    "    for doc in documents:\n",
    "        doc_lower = doc.lower()\n",
    "        query_words = query.lower().split()\n",
    "\n",
    "        for word in query_words:\n",
    "            if word in doc_lower:\n",
    "                matching_docs.append(doc)\n",
    "                break  # Stop checking after the first match\n",
    "\n",
    "    return matching_docs\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the prompt for generation, and then generating the response\n",
    "- Creating the context with the retrieved documents, and query question\n",
    "- Creating a pipeline object, where we define the model we want to use for generation, we are using `GPT-2` here\n",
    "- We pass the prompt and the parameters such as `max_new_tokens`, which instructs model about the number of tokens to use in the generation. More `max_new_tokens`, more are the chances of the model not making much sense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " \n",
      "\n",
      "User Query: \n",
      "What are the side effects of Metformin? \n",
      "\n",
      "Matched documents: \n",
      "['It works by lowering glucose production in the liver.', 'Common side effects include nausea, upset stomach, and diarrhea.']\n",
      "\n",
      "Generated Response \n",
      " \n",
      "\n",
      "Context: It works by lowering glucose production in the liver.\\nCommon side effects include nausea, upset stomach, and diarrhea.\\n\\nQuestion: What are the side effects of Metformin?\\nAnswer: The side effect profile may include: severe abdominal pain, nausea, and vomiting related to ingestion of Metformin.\\n\\nDiagnosis: Metformin is well tolerated.\\n\\nDiagnosis should be made if the gastrointestinal side effect occurs.\\n\\nEfficacy: Metformin is used to treat a number of gastrointestinal symptoms as described above.\\n\\nOther side effects include: severe diarrhea, constipation, vomiting\n",
      "\n",
      "The side effects \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Combine query with retrieved docs\n",
    "def create_prompt(query, retrieved_docs):\n",
    "    context = \"\\\\n\".join(retrieved_docs)\n",
    "    return f\"Context: {context}\\\\n\\\\nQuestion: {query}\\\\nAnswer:\"\n",
    "\n",
    "\n",
    "# Step 4: Use an LLM to generate answer\n",
    "qa_pipeline = pipeline(\"text-generation\", model=\"gpt2\")\n",
    "print(\"\\n \\n\")\n",
    "#query\n",
    "query = \"What are the side effects of Metformin?\"\n",
    "\n",
    "# matched documents\n",
    "retrieved = retrieve_docs(query, knowledge_base)\n",
    "\n",
    "# prompt that packs the query + matched documents\n",
    "prompt = create_prompt(query, retrieved)\n",
    "\n",
    "# generation\n",
    "result = qa_pipeline(prompt, max_new_tokens=100, do_sample=True)[0]['generated_text']\n",
    "\n",
    "\n",
    "print(\"User Query: \")\n",
    "print(query, '\\n')\n",
    "print('Matched documents: ')\n",
    "print(retrieved)\n",
    "print()\n",
    "print(\"Generated Response \\n \\n\")\n",
    "print(result, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
