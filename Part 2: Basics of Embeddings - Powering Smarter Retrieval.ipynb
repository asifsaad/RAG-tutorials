{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Semantic Embedding using HuggingFace's Sentence Trasformers library (The Easy way)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentence-transformers\n",
      "  Downloading sentence_transformers-4.0.1-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in ./rag_env/lib/python3.12/site-packages (from sentence-transformers) (4.50.1)\n",
      "Requirement already satisfied: tqdm in ./rag_env/lib/python3.12/site-packages (from sentence-transformers) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in ./rag_env/lib/python3.12/site-packages (from sentence-transformers) (2.6.0)\n",
      "Collecting scikit-learn (from sentence-transformers)\n",
      "  Using cached scikit_learn-1.6.1-cp312-cp312-macosx_12_0_arm64.whl.metadata (31 kB)\n",
      "Collecting scipy (from sentence-transformers)\n",
      "  Using cached scipy-1.15.2-cp312-cp312-macosx_14_0_arm64.whl.metadata (61 kB)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in ./rag_env/lib/python3.12/site-packages (from sentence-transformers) (0.29.3)\n",
      "Collecting Pillow (from sentence-transformers)\n",
      "  Using cached pillow-11.1.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (9.1 kB)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in ./rag_env/lib/python3.12/site-packages (from sentence-transformers) (4.13.0)\n",
      "Requirement already satisfied: filelock in ./rag_env/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./rag_env/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.0)\n",
      "Requirement already satisfied: packaging>=20.9 in ./rag_env/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./rag_env/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
      "Requirement already satisfied: requests in ./rag_env/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
      "Requirement already satisfied: networkx in ./rag_env/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in ./rag_env/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
      "Requirement already satisfied: setuptools in ./rag_env/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (78.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in ./rag_env/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./rag_env/lib/python3.12/site-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: numpy>=1.17 in ./rag_env/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.2.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./rag_env/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in ./rag_env/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in ./rag_env/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn->sentence-transformers)\n",
      "  Using cached joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn->sentence-transformers)\n",
      "  Using cached threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./rag_env/lib/python3.12/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./rag_env/lib/python3.12/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./rag_env/lib/python3.12/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./rag_env/lib/python3.12/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./rag_env/lib/python3.12/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.1.31)\n",
      "Downloading sentence_transformers-4.0.1-py3-none-any.whl (340 kB)\n",
      "Using cached pillow-11.1.0-cp312-cp312-macosx_11_0_arm64.whl (3.1 MB)\n",
      "Using cached scikit_learn-1.6.1-cp312-cp312-macosx_12_0_arm64.whl (11.2 MB)\n",
      "Using cached scipy-1.15.2-cp312-cp312-macosx_14_0_arm64.whl (22.4 MB)\n",
      "Using cached joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Using cached threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, scipy, Pillow, joblib, scikit-learn, sentence-transformers\n",
      "Successfully installed Pillow-11.1.0 joblib-1.4.2 scikit-learn-1.6.1 scipy-1.15.2 sentence-transformers-4.0.1 threadpoolctl-3.6.0\n"
     ]
    }
   ],
   "source": [
    "! pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/saadasif/MyShelf/CODE_REPO/RAG-tutorial-series/rag_env/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Representing Sentences into Dense vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here we are mere using the SentenceTransformers module\n",
    "- `SentenceTransformer` module, provides access to the pre-trained models for generating sentence embeddings\n",
    "- The class provided by SentenceTransformer is built on top of the HuggingFace's Transformers and PyTorch/TensorFlow\n",
    "\n",
    "### About the model Chosen here\n",
    "- MiniLM-L6-v2 is a small efficient version of the BERT like model\n",
    "- L6 in the name implies it has 6 layers, and v2 implies this is version 2\n",
    "- It has been optimized for speed and memory, making it a good choice for generating the embeddings\n",
    "- It maps sentences into 384-dimension embeddings ie numberical representation of the input sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load a pre-trained embedding model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the document corpus\n",
    "- The documents defined below would serve as the corpus the Retriever module would siff throuh\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example sentences\n",
    "sentences = [\n",
    "    \"Metformin is a medication used to treat type 2 diabetes.\",\n",
    "    \"Common side effects include nausea, upset stomach, and diarrhea.\",\n",
    "    \"Insulin helps control blood sugar levels.\",\n",
    "    \"Octopuses have three hearts and blue blood.\",\n",
    "    \"Bananas are berries, but strawberries are not.\",\n",
    "    \"The Eiffel Tower can grow more than 6 inches in summer due to heat expansion.\",\n",
    "    \"Honey never spoils and has been found edible in ancient Egyptian tombs.\",\n",
    "    \"Sharks have been around longer than trees, existing for over 400 million years.\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding the document corpus\n",
    "- The model corpus is encoded, using the model defined by the `model` pipeline variable\n",
    "- This would output the corpus into vector representation, based on the specifications of the model\n",
    "- Although the actual dimensions of the output vector is 384-dimension, we are just looking at the first 5 dimensions of the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embeddings shape -> (num_sentences, embedding_dimensions) : (8, 384)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.06462408, -0.01414089, -0.04102236,  0.05251157, -0.07824506],\n",
       "       [ 0.02023205, -0.05939261,  0.00395693,  0.02894642, -0.00524463],\n",
       "       [-0.07433561,  0.1275397 , -0.06659397,  0.06253764, -0.00612355],\n",
       "       [ 0.01835446,  0.01416124, -0.01720773,  0.04222286, -0.07529111],\n",
       "       [ 0.016516  , -0.0464867 ,  0.00407454,  0.01632662,  0.01158659],\n",
       "       [ 0.02896879,  0.00143153,  0.01369293,  0.04595578,  0.00489472],\n",
       "       [-0.04607044,  0.0279206 ,  0.04649594, -0.00482514, -0.01890971],\n",
       "       [-0.01410103,  0.02397047,  0.05505345,  0.05128846,  0.00436963]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Using the Model Pipeline to generate embeddings\n",
    "\n",
    "# Generate embeddings\n",
    "embeddings = model.encode(sentences)\n",
    "print(f\"embeddings shape -> (num_sentences, embedding_dimensions) : {embeddings.shape}\")\n",
    "embeddings[:, :5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output Embeddings Shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of dense vector representation of 8 sentences : {embeddings.shape}\n",
      "\n",
      "[-0.06462408 -0.01414089 -0.04102236  0.05251157 -0.07824506  0.00645232\n",
      "  0.01479468  0.10869684  0.02307448 -0.05817275]\n",
      "[ 0.02023205 -0.05939261  0.00395693  0.02894642 -0.00524463 -0.0390905\n",
      "  0.04136479  0.08110133 -0.03313445 -0.04686553]\n"
     ]
    }
   ],
   "source": [
    "## Each of the sentences have been transformed into a \n",
    "print(\"Shape of dense vector representation of 8 sentences : {embeddings.shape}\")\n",
    "print()\n",
    "## First 10 dimensions of the dense representation of sentence 1 and 2.\n",
    "print(embeddings[0][:10])\n",
    "print(embeddings[1][:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimenting with Semantic Similarity\n",
    "- We have multi-dimension representation of the output vectors\n",
    "- We check the similarity scores between the embeddings representation of the output vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metformin is a medication used to treat type 2 diabetes.\n",
      "Insulin helps control blood sugar levels.\n",
      "similarity score : 0.3995707333087921\n"
     ]
    }
   ],
   "source": [
    "## semantic similarity between sentences\n",
    "print(sentences[0])\n",
    "print(sentences[2])\n",
    "print(f\"similarity score : {np.dot(embeddings[0], embeddings[2])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measuring the Semantic Similarities between embedded sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metformin is a medication used to treat type 2 diabetes.\n",
      "Octopuses have three hearts and blue blood.\n",
      "similarity score : -0.016284015029668808\n"
     ]
    }
   ],
   "source": [
    "## semantic similarity between sentences\n",
    "print(sentences[0])\n",
    "print(sentences[3])\n",
    "print(f\"similarity score : {np.dot(embeddings[0], embeddings[3])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternatively, using cosine similarity metric from sklearn.metrics, to measure how similar the vectors are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity Score: 0.0351\n"
     ]
    }
   ],
   "source": [
    "# Compare the first and second sentences\n",
    "sim = cosine_similarity([embeddings[0]], [embeddings[1]])\n",
    "print(f\"Similarity Score: {sim[0][0]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (OPTIONAL) Generating Semantic Embedding using PyTorch (the hard way)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Closer Look at the inner workings of SentenceTransformer Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetching the model/tokenizer\n",
    "\n",
    "### Tokenizer\n",
    "- AutoTokenizer() helps you load the given tokenizer object for a specific model\n",
    "- A tokenizer in LLMs breaks down raw text into smaller units called tokens (e.g., words, subwords, or characters), which the model can understand and process. \n",
    "- It also handles the reverse operation, converting generated tokens back into human-readable text.\n",
    "\n",
    "### Model\n",
    "- The `model` object holds, a pre-trained neural network loaded from the Hugging Face Transformers library based on the specified model_name. \n",
    "- It provides the architecture and weights needed to put together the model in memory, to the generate embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loading the tokenizer and model (MiniLM)\n",
    "\n",
    "model_name = 'sentence-transformers/all-MiniLM-L6-v2'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer object\n",
    "- Expains the model name\n",
    "- Vocabulary size\n",
    "- Special tokens\n",
    "- Padding process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertTokenizerFast(name_or_path='sentence-transformers/all-MiniLM-L6-v2', vocab_size=30522, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Object\n",
    "\n",
    "- Describes the structure of the model loaded\n",
    "- The model structure, in this case its the Encoder-only model\n",
    "- It also describes the output dimensions of each of the layer\n",
    "- Shows the Attention Mechanism - with the Key, Query and Value matrices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 384, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 384)\n",
       "    (token_type_embeddings): Embedding(2, 384)\n",
       "    (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-5): 6 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSdpaSelfAttention(\n",
       "            (query): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (key): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (value): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=384, out_features=384, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Metformin is a medication used to treat type 2 diabetes.',\n",
       " 'Common side effects include nausea, upset stomach, and diarrhea.',\n",
       " 'Insulin helps control blood sugar levels.',\n",
       " 'Octopuses have three hearts and blue blood.',\n",
       " 'Bananas are berries, but strawberries are not.',\n",
       " 'The Eiffel Tower can grow more than 6 inches in summer due to heat expansion.',\n",
       " 'Honey never spoils and has been found edible in ancient Egyptian tombs.',\n",
       " 'Sharks have been around longer than trees, existing for over 400 million years.']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Raw input -> Tokenized Input\n",
    "The tokenizer:\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  101,  2777, 14192,  2378,  2003,  1037, 14667,  2109,  2000,  7438,\n",
       "          2828,  1016, 14671,  1012,   102,     0,     0,     0,     0,     0],\n",
       "        [  101,  2691,  2217,  3896,  2421, 19029,  1010,  6314,  4308,  1010,\n",
       "          1998, 22939, 12171, 20192,  1012,   102,     0,     0,     0,     0],\n",
       "        [  101, 22597,  7126,  2491,  2668,  5699,  3798,  1012,   102,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
       "        [  101, 24318,  2229,  2031,  2093,  8072,  1998,  2630,  2668,  1012,\n",
       "           102,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
       "        [  101, 26191,  2024, 22681,  1010,  2021, 13137, 20968,  2024,  2025,\n",
       "          1012,   102,     0,     0,     0,     0,     0,     0,     0,     0],\n",
       "        [  101,  1996,  1041, 13355,  2884,  3578,  2064,  4982,  2062,  2084,\n",
       "          1020,  5282,  1999,  2621,  2349,  2000,  3684,  4935,  1012,   102],\n",
       "        [  101,  6861,  2196, 27594,  2015,  1998,  2038,  2042,  2179, 21006,\n",
       "          1999,  3418,  6811, 16623,  1012,   102,     0,     0,     0,     0],\n",
       "        [  101, 12004,  2031,  2042,  2105,  2936,  2084,  3628,  1010,  4493,\n",
       "          2005,  2058,  4278,  2454,  2086,  1012,   102,     0,     0,     0]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenize and encode the inputs\n",
    "\n",
    "inputs = tokenizer(sentences,\n",
    "                   padding = True,\n",
    "                   truncation = True,\n",
    "                   return_tensors = \"pt\")\n",
    "\n",
    "# tokenized input\n",
    "inputs['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[-1.1995e-01, -3.3542e-01, -2.0211e-01,  ..., -2.8208e-01,\n",
       "          -3.1256e-01, -1.3458e-01],\n",
       "         [-8.2270e-01, -1.3833e-01,  1.6015e-01,  ..., -4.8873e-01,\n",
       "          -9.3062e-01,  7.0992e-01],\n",
       "         [-6.6834e-01, -5.9575e-03,  2.7987e-01,  ...,  5.8347e-01,\n",
       "           5.6502e-01, -1.6965e-01],\n",
       "         ...,\n",
       "         [-6.4299e-01, -3.9824e-01,  2.2439e-02,  ..., -5.1957e-01,\n",
       "           4.2425e-01, -1.9175e-01],\n",
       "         [-6.3898e-01, -4.1930e-01,  4.4994e-02,  ..., -4.9156e-01,\n",
       "           4.2146e-01, -2.5802e-01],\n",
       "         [-6.2997e-01, -4.1032e-01,  5.0820e-02,  ..., -4.8530e-01,\n",
       "           3.7536e-01, -2.7483e-01]],\n",
       "\n",
       "        [[ 4.5147e-01,  4.4765e-02,  1.4672e-01,  ...,  6.1254e-02,\n",
       "          -4.0575e-01, -1.1386e-01],\n",
       "         [ 3.8334e-01, -3.6245e-01,  5.4244e-01,  ...,  4.6947e-01,\n",
       "          -1.1225e+00,  8.0497e-01],\n",
       "         [-8.2962e-01, -1.2266e-02,  2.8013e-01,  ...,  6.9566e-01,\n",
       "          -1.3816e+00,  5.5598e-01],\n",
       "         ...,\n",
       "         [ 1.0981e-01, -3.8615e-01,  1.7520e-01,  ..., -3.1967e-01,\n",
       "           2.0475e-01, -1.6537e-01],\n",
       "         [ 9.0478e-02, -4.4266e-01,  2.1981e-01,  ..., -2.8117e-01,\n",
       "           7.8183e-02, -2.0606e-01],\n",
       "         [ 8.1908e-02, -4.4221e-01,  2.4018e-01,  ..., -2.8088e-01,\n",
       "          -2.2266e-02, -2.2328e-01]],\n",
       "\n",
       "        [[-3.1651e-01,  4.6889e-01, -6.1999e-01,  ..., -9.6785e-02,\n",
       "          -1.2512e-01, -2.5101e-01],\n",
       "         [-5.8519e-01,  1.6805e+00, -1.0797e+00,  ..., -1.7208e-01,\n",
       "           9.1711e-01, -1.6448e+00],\n",
       "         [-6.9734e-01,  1.0580e+00,  9.1311e-02,  ...,  9.9723e-01,\n",
       "           8.6154e-01, -2.9769e-01],\n",
       "         ...,\n",
       "         [-4.3097e-01,  1.6329e-02, -4.0888e-02,  ..., -7.3529e-02,\n",
       "           5.1228e-01, -3.1034e-01],\n",
       "         [-3.9358e-01, -2.9966e-02, -2.5526e-02,  ..., -1.0053e-01,\n",
       "           4.8398e-01, -3.4605e-01],\n",
       "         [-3.6045e-01, -2.5199e-02, -5.8578e-03,  ..., -1.4523e-01,\n",
       "           4.0045e-01, -3.6444e-01]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 1.4273e-01,  1.2085e-01,  1.8771e-01,  ...,  6.0025e-02,\n",
       "           1.3614e-01, -3.0254e-02],\n",
       "         [ 2.1677e-01,  1.5526e-01,  6.7734e-02,  ..., -2.3584e-02,\n",
       "           3.7061e-01,  9.4363e-01],\n",
       "         [ 2.2682e-02,  7.4542e-02,  4.9429e-01,  ...,  1.8172e-01,\n",
       "          -1.6617e-01,  3.3535e-01],\n",
       "         ...,\n",
       "         [-9.7965e-03, -1.4105e-01,  2.7943e-01,  ..., -1.5645e-02,\n",
       "           1.5076e-01,  3.6413e-01],\n",
       "         [ 3.3729e-01,  9.2211e-02,  1.3009e-01,  ..., -7.9949e-02,\n",
       "           1.7629e-01,  4.4371e-01],\n",
       "         [ 3.7147e-01,  2.0524e-02,  1.0699e-01,  ..., -6.4359e-02,\n",
       "           1.7158e-01,  5.5204e-01]],\n",
       "\n",
       "        [[-3.2389e-01,  9.5654e-02,  3.9111e-01,  ..., -4.9968e-02,\n",
       "          -4.0997e-01, -3.7354e-01],\n",
       "         [ 1.5189e-01, -3.0122e-01,  4.2912e-01,  ..., -1.9979e-01,\n",
       "          -5.1757e-01, -1.1662e+00],\n",
       "         [-1.5602e-01,  1.2908e-02,  4.1431e-01,  ..., -3.2647e-01,\n",
       "           7.5492e-02,  3.1473e-04],\n",
       "         ...,\n",
       "         [-3.2342e-01, -1.4760e-01,  4.4837e-01,  ..., -2.3607e-01,\n",
       "           1.3430e-01,  2.5355e-02],\n",
       "         [-3.1958e-01, -1.9862e-01,  4.4490e-01,  ..., -1.4539e-01,\n",
       "           7.7549e-02, -4.0101e-02],\n",
       "         [-3.4355e-01, -1.8429e-01,  4.2287e-01,  ..., -1.1693e-01,\n",
       "           1.1215e-02, -5.3690e-02]],\n",
       "\n",
       "        [[ 3.5135e-02,  7.6370e-03,  2.9145e-01,  ..., -3.0547e-01,\n",
       "           2.1286e-01,  3.7127e-02],\n",
       "         [-6.6198e-01,  2.8474e-02,  2.0042e-02,  ..., -1.5021e-01,\n",
       "           1.4544e+00,  3.4964e-01],\n",
       "         [ 8.5256e-02, -2.4162e-01,  5.2329e-01,  ..., -4.9471e-01,\n",
       "           2.0940e-01,  3.5500e-01],\n",
       "         ...,\n",
       "         [ 4.4518e-02, -9.1946e-02,  4.2472e-01,  ..., -2.1920e-01,\n",
       "           5.5347e-01, -5.1593e-02],\n",
       "         [ 6.0017e-02, -1.4830e-01,  4.7256e-01,  ..., -1.2754e-01,\n",
       "           4.2112e-01, -3.6812e-02],\n",
       "         [ 3.1467e-02, -1.5170e-01,  4.8805e-01,  ..., -5.1007e-02,\n",
       "           2.6345e-01,  1.4331e-02]]]), pooler_output=tensor([[-0.0311, -0.0630, -0.0064,  ...,  0.0474, -0.1095, -0.0110],\n",
       "        [-0.0433,  0.0411, -0.0670,  ..., -0.0327, -0.0914, -0.0869],\n",
       "        [-0.0478, -0.0193, -0.0050,  ...,  0.0198, -0.0631,  0.0224],\n",
       "        ...,\n",
       "        [-0.0565, -0.0012, -0.0040,  ...,  0.1031,  0.0038, -0.0744],\n",
       "        [ 0.0538, -0.0415,  0.0220,  ..., -0.0438, -0.0626,  0.0034],\n",
       "        [-0.1058,  0.0499, -0.0015,  ..., -0.0430, -0.0292, -0.0472]]), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "## Getting the input to pass through the loaded model\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs =  model(**inputs)\n",
    "\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 20, 384])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Accessing the model layer where \n",
    "##\n",
    "\n",
    "outputs.last_hidden_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
